{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "--- \n",
    "title: Forklift detection system. \n",
    "authors: \n",
    "- Alejandro Torrado Pazos\n",
    "- Javier Esteban Quiles\n",
    "tags: \n",
    "- Computer vision \n",
    "- YOLO (You Only Look Once)\n",
    "- Object detection\n",
    "created_at: 2019-08-02 \n",
    "updated_at: 2019-08-02 \n",
    "tldr: Incident Classification. \n",
    "thumbnail: images/xx.png \n",
    "--- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Digital case repository: [https://repsol-digital-team.visualstudio.com/CoreDataScienceTeam/_git/CognitiveChallenge2019/]) \n",
    "- Model Catalog: [https://repsol-digital-team.visualstudio.com/CoreDataScienceTeam/_git/CognitiveChallenge2019/]\n",
    "    <br>\n",
    "    <font color='red'> link a la implementación concreta de YOLOv3 usada?</font>\n",
    "- Data files: available at [this page]() - x files, xx total size\n",
    "    <br>\n",
    "    <font color='red'> Dataset de imágenes del entrenamiento? </font>\n",
    "- LeanKit story: https://repsolqm.leankit.com/card/902249021\n",
    "- Reviewers: Elena Tomás"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXECUTIVE SUMMARY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following report describes the methods tested and used to detect forklifts and walking operators in Repsol butane cylinder factories in order to avoid accidents involving them (running overs, in particular).\n",
    "\n",
    "The objective was to use the live signal of a suitably placed camera to detect the position of each category (forklifts and walking operators) and get the distance between them so that, if it is smaller than a certain limit, the system warns they are dangerously close, thus avoiding the accident. In order for this to work properly the detection and distance calculation needed to be as close to real time as possible.\n",
    "\n",
    "Below are the final results that gave the best accuracy, as well as a description of the method used for these results to demonstrate the (current) most useful tools for this case.\n",
    "\n",
    "We do not have access to the camera(s) yet, although we have been provided with 3 pictures and 2 videos taken in the factory. \n",
    "\n",
    "\n",
    "*****\n",
    "[Videos de 30 segundos]. \n",
    "[Cantidad de muestras insuficientes para entrenar un modelo satisfactorio]\n",
    "[Las carretillas estaban mas o menos siempre en la misma posicion. Habia pocas perspectivas distintas]\n",
    "\n",
    "*****\n",
    "\n",
    "the model has been tested on images and videos we found over the Internet.\n",
    "\n",
    "In the project there have been two main lines of work:\n",
    "- ...\n",
    "- ...\n",
    "\n",
    "*****\n",
    "[Aqui ya no se que tienes en mente. Por poner una propuesta yo diria que son 3 fases pero en la misma linea (no 2 paralelas): \n",
    "    [Entrenar un modelo que detecte carretillas con imagenes de google. Fue el que enseñamos en la presentacion]\n",
    "    [Entrenar un modelo con las imagenes del almacen. No fue posible. Pero se intento, se etiquetaron las imagenes razonando distintas opciones ]\n",
    "    [Streaming detectando coches y personas y dibujando la linea entre ellos]\n",
    "\n",
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Model research and final choice: An object detection model called YOLO was finally chosen for accuracy and speed reasons, as explained in the Scientific Report.\n",
    "2. Our first idea was to take advantage of the fact that YOLO is already pre-trained and is capable of recognizing cars and trucks among many things, so we checked if it is able to detect forklifts as one of them (taking into account the similarities: they all have wheels, a more or less similar shape...). The idea didn't work well, so we moved on to preparing a dataset which we could train our model with.\n",
    "3. /..../We constructed our dataset with images from the web, some of them being from the ImageNet dataset./..../\n",
    "4. We used a program called VoTT to make annotations (meaning drawing boxes around the forklifts and labeling them as such) on the images mentioned above.\n",
    "5. Using the annotated images we could train our first model, YOLOv3.\n",
    "6. YOLOv3 is very powerful and precise in its predictions but it also requires large computational resources, so we had to use a virtual machine. Apart from that, we have also been using a faster but less accurate model, called tiny-YOLOv3, to work with less powerful devices like our personal computers.\n",
    "7. \n",
    "8. .........<font color='red'>presentación que hicimos¿?</font>\n",
    "9. .........<font color='red'>web, streaming, streaming + web¿?</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCIENTIFIC REPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, we don't have access to the live signal from the factory cameras but we have been given pictures and videos that have been taken there. From this point on we followed two different lines of work:\n",
    "1. We used the data (images and videos) provided to train the model but there were a series of problems: The videos were around 30 seconds long but the forklifts were mostly in the same position. There were few different perspectives so we had an insufficient number of samples to train a satisfactory model.  <br>\n",
    "\n",
    "2. We constructed our own dataset with images from the web, which we trained our model with. This dataset consists of around 400 images, each one showing between 1 and 3 forklifts in different positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Pre-Processing /.../ <font color='red'> Image Annotation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image annotation process basically consists of drawing boxes around the object category we want our model to be able to detect after the training.\n",
    "\n",
    "Regarding the videos and pictures given to us, we used VoTT to draw boxes and label each picture and around 1 in 10 frames for each video. However, there were two main options, two main ways of doing it:\n",
    "\n",
    "- __Option (a):__ Distinguishing the load and the forklift as distinct objects.\n",
    "- __Option (b):__ The entire forklift, be it loaded or not, is boxed (if it is carrying a load, it is labeled in one way and, otherwise, in another). In this way the process of calculating the distance is only done with respect to 1 object, not 2. This second way of making annotations is shown in the picture below:\n",
    "<img src=\"opcion b.png\">\n",
    "\n",
    "The chosen option was the second one, option (b). The main reasons are:\n",
    "- We are (or will be) working with images from a butane cylinder factory in which hundreds of them are stacked, so, if we followed the first option, our model would probably detect all of them, not only the ones being carried by forklifts. This won't work as needed and, in fact, it would take much longer to be accomplished.\n",
    "- The second option ensures the correct detection and calculation of distances regardless of whether the forklift is loaded or not.\n",
    "\n",
    "Regarding the images used for building up our dataset the criteria we followed was to use VoTT to box the forklift itself, using the same label regardless of whether it is loaded or not. This was necessary because the forklifts in that dataset carry different objects (or none at all) and this could make training and detection more difficult, causing the model to be less accurate.<font color='red'>...¿?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: YOLOv3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we used the complete third version of YOLO, called YOLOv3, which is very well designed to work close to real time in object detecion problems. It is based on a convolutional neural network called DarkNet-53 which, as its name may suggest, consists of 53 convolutional layers. The complete network YOLOv3 uses consists of 106 layers in total, 75 of which are convolutional layers (https://arxiv.org/abs/1804.02767). Among this model's features the most interesting and useful for this project are the multi-label prediction and the multi-scale prediction, thanks to which it can detect the same object at different distances and scales.\n",
    "\n",
    "This is the model we have used for the streaming part of the project. We relied on the GPU of the virtual machine that was assigned to us to process the images as fast as possible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...¿?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: YOLOv3-Tiny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detection with YOLOv3 wat done at about 2 fps in our personal computers, which was clearly not enough to meet the targets. As we have already mentioned, we used a smaller, less accurate but faster version of YOLOv3, called tiny YOLOv3, while working with them. It reduces the number of convolutional layers. Its basic structure (the feature extractor for training) has 7 convolutional layers instead of 53. To reduce dimensionality it also uses pooling layers instead of the other convolutional layers YOLOv3 uses.\n",
    "\n",
    "This is the model we have trained using the dataset constructed with images found over the internet. We used this model to make the presentation but then we moved on to the streaming part of the project, which uses YOLOv3, as mentioned above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
